# -*- coding: utf-8 -*-
"""Central Black 003

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XsEy52zDlmW3HVze_aIL0s7yCqoKJIK0
"""

# -*- coding: utf-8 -*-
"""Central_black_002

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SRvjkV_Ax7S7tmhgv-JUV97Zr4m8gkf1
"""

import os

try:
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive montado correctamente.")
except ModuleNotFoundError:
    print("Error: El módulo 'google.colab' no se encontró.")
    print("Este cuaderno está diseñado para ejecutarse en Google Colab.")
    print("Asegúrate de estar ejecutando este archivo en un entorno de Google Colab.")
    print("La montura de Google Drive es necesaria para acceder a los archivos del proyecto.")
except Exception as e:
    print(f"Error al montar Google Drive: {e}")
    print("Por favor, verifica tu conexión a Google Drive y los permisos.")

with open("/content/drive/MyDrive/Central_Black/colab context.txt", 'r', encoding='latin-1') as f:
  colab_context = f.read()

with open("/content/drive/MyDrive/Central_Black/plan context.txt", 'r', encoding='latin-1') as f:
  plan_context = f.read()

print("colab context.txt content:")
print(colab_context)
print("\nplan context.txt content:")
print(plan_context)

"""# Sección nueva

## Fase I: Configuración del Entorno en Colab

**Instrucción 1.1: Descompresión e Instalación de Dependencias**
"""

import pandas as pd
import numpy as np
import subprocess
import sys
import os

# Instalar dependencias del Backend
print("Instalando dependencias del Backend...")
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "fastapi[all]", "python-dotenv", "google-generativeai", "requests", "pyngrok", "uvicorn", "transformers"])
print("Dependencias del Backend instaladas.")

# Descomprimir el proyecto (movido de la celda 3 para asegurar que las rutas existan para npm install)
zip_path = "/content/drive/MyDrive/Central_Black/Project_Central_black.zip"
extract_path = "/content/central_black_extracted"
os.makedirs(extract_path, exist_ok=True)
try:
    subprocess.check_call(["unzip", "-q", "-o", zip_path, "-d", extract_path])
    print(f"Archivo zip descomprimido en: {extract_path}")
except FileNotFoundError:
    print(f"Error: El comando 'unzip' no se encontró. Asegúrate de que esté disponible en el entorno.")
except subprocess.CalledProcessError as e:
    print(f"Error al descomprimir el archivo zip: {e}")
except Exception as e:
    print(f"Ocurrió un error durante la descompresión: {e}")

# Instalar dependencias del Frontend
print("Instalando dependencias del Frontend...")
frontend_path = "/content/central_black_extracted/1_frontend_codeforge"
if os.path.exists(frontend_path):
    current_dir = os.getcwd()
    os.chdir(frontend_path)
    try:
        subprocess.check_call(["npm", "install"])
        print("Dependencias del Frontend instaladas.")
    except FileNotFoundError:
        print(f"Error: El comando 'npm' no se encontró. Asegúrate de que Node.js y npm estén instalados.")
    except subprocess.CalledProcessError as e:
        print(f"Error al instalar dependencias del Frontend: {e}")
    except Exception as e:
        print(f"Ocurrió un error durante la instalación del Frontend: {e}")
    os.chdir(current_dir)
else:
    print(f"Error: Directorio del frontend no encontrado en {frontend_path}. Asegúrate de que el zip contenga esta carpeta.")

# Asegurar que estamos en el directorio correcto para el backend después de la instalación del frontend
backend_orchestrator_path = "/content/central_black_extracted/2_backend_orchestrator"
if os.path.exists(backend_orchestrator_path):
    os.chdir(backend_orchestrator_path)
    print(f"Cambiado al directorio del backend: {os.getcwd()}")
else:
    print(f"Advertencia: El directorio del backend {backend_orchestrator_path} no existe. Por favor, verifica la estructura del zip.")


num_rows = 1000000

states = ["NY", "NJ", "CA", "TX"]
violations = ["Double Parking", "Expired Meter", "No Parking", "Fire Hydrant", "Bus Stop"]
vehicle_types = ["SUBN", "SDN"]

start_date = "2022-01-01"
end_date = "2022-12-31"
dates = pd.date_range(start=start_date, end=end_date, freq='D')

data = {
    "Registration State": np.random.choice(states, size=num_rows),
    "Violation Description": np.random.choice(violations, size=num_rows),
    "Vehicle Body Type": np.random.choice(vehicle_types, size=num_rows),
    "Issue Date": np.random.choice(dates, size=num_rows),
    "Ticket Number": np.random.randint(1000000000, 9999999999, size=num_rows)
}

df = pd.DataFrame(data)

weekday_names = {
    0: "Monday",
    1: "Tuesday",
    2: "Wednesday",
    3: "Thursday",
    4: "Friday",
    5: "Saturday",
    6: "Sunday",
}

df["issue_weekday"] = df["Issue Date"].dt.weekday.map(weekday_names)

df.groupby(["Issue Date"])["Ticket Number"
].count().sort_values()

import pandas as pd
import numpy as np

num_rows = 1000000

states = ["NY", "NJ", "CA", "TX"]
violations = ["Double Parking", "Expired Meter", "No Parking",
              "Fire Hydrant", "Bus Stop"]
vehicle_types = ["SUBN", "SDN"]

start_date = "2022-01-01"
end_date = "2022-12-31"
dates = pd.date_range(start=start_date, end=end_date, freq='D')

data = {
    "Registration State": np.random.choice(states, size=num_rows),
    "Violation Description": np.random.choice(violations, size=num_rows),
    "Vehicle Body Type": np.random.choice(vehicle_types, size=num_rows),
    "Issue Date": np.random.choice(dates, size=num_rows),
    "Ticket Number": np.random.randint(1000000000, 9999999999, size=num_rows)
}

df = pd.DataFrame(data)

(df[["Registration State", "Violation Description"]]
 .value_counts()
 .groupby("Registration State")
 .head(1)
 .sort_index()
 .reset_index()
)

import pandas as pd
import numpy as np

num_rows = 1000000

states = ["NY", "NJ", "CA", "TX"]
violations = ["Double Parking", "Expired Meter", "No Parking",
              "Fire Hydrant", "Bus Stop"]
vehicle_types = ["SUBN", "SDN"]

start_date = "2022-01-01"
end_date = "2022-12-31"
dates = pd.date_range(start=start_date, end=end_date, freq='D')

data = {
    "Registration State": np.random.choice(states, size=num_rows),
    "Violation Description": np.random.choice(violations, size=num_rows),
    "Vehicle Body Type": np.random.choice(vehicle_types, size=num_rows),
    "Issue Date": np.random.choice(dates, size=num_rows),
    "Ticket Number": np.random.randint(1000000000, 9999999999, size=num_rows)
}

df = pd.DataFrame(data)

(df[["Registration State", "Violation Description"]]
 .value_counts()
 .groupby("Registration State")
 .head(1)
 .sort_index()
 .reset_index()
)

"""**2. Configurar ngrok**

Asegúrate de haber guardado tu authtoken de ngrok v2 en Colab Secrets con el nombre `NGROK_AUTH_TOKEN`.
"""

from google.colab import userdata
import os
from pyngrok import ngrok

try:
    NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')
    if NGROK_AUTH_TOKEN:
        ngrok.set_auth_token(NGROK_AUTH_TOKEN)
        os.environ["NGROK_AUTH_TOKEN"] = NGROK_AUTH_TOKEN
        print("ngrok authtoken loaded from Secrets and configured.")
    else:
        print("Error: NGROK_AUTH_TOKEN not found in Colab Secrets. Please add it using the key icon on the left.")
except Exception as e:
    print(f"An error occurred while loading ngrok authtoken from Secrets: {e}")
    print("Please ensure NGROK_AUTH_TOKEN is added to Colab Secrets.")

"""**3. Descomprimir el Proyecto y Crear Directorios**"""

import zipfile
import os
import subprocess

zip_path = "/content/drive/MyDrive/Central_Black/Project_Central_black.zip"
extract_path = "/content/central_black_extracted"

os.makedirs(extract_path, exist_ok=True)

try:
    # La descompresión se movió a la celda 1.1 para asegurar la disponibilidad de rutas para npm install.
    # Este bloque ahora solo se asegura de que los directorios del backend estén creados.

    base_path = "/content/central_black_extracted/2_backend_orchestrator"
    app_path = os.path.join(base_path, "app")
    core_path = os.path.join(app_path, "core")
    schemas_path = os.path.join(app_path, "schemas")
    services_path = os.path.join(app_path, "services")
    endpoints_path = os.path.join(app_path, "api/v1/endpoints")
    agents_path = os.path.join(base_path, "agents")

    os.makedirs(core_path, exist_ok=True)
    os.makedirs(schemas_path, exist_ok=True)
    os.makedirs(services_path, exist_ok=True)
    os.makedirs(endpoints_path, exist_ok=True)
    os.makedirs(agents_path, exist_ok=True)

    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/.env"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/core/config.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/schemas/orchestrate.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/services/gemini_service.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/services/ollama_service.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/services/coreml_service.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/services/huggingface_service.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/agents/app_agent_orchestrator.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/api/v1/endpoints/orchestrate.py"), exist_ok=True)
    os.makedirs(os.path.dirname("/content/central_black_extracted/2_backend_orchestrator/app/main.py"), exist_ok=True)

    backend_dir = "/content/central_black_extracted/2_backend_orchestrator"
    os.makedirs(backend_dir, exist_ok=True)
    print(f"Se aseguró que el directorio existe: {backend_dir}")


    print("Directorios creados.")

except Exception as e:
    print(f"Ocurrió un error durante la creación de directorios: {e}")


"""**4. Escribir Archivos del Backend**"""

# Escribir .env
with open("/content/central_black_extracted/2_backend_orchestrator/.env", "w", encoding="utf-8") as f:
    f.write("""GEMINI_API_KEY="AIzaSyAks19A_5BCKXsR4aMlvBtjEQAnr6lTOLw"
OLLAMA_HOST="http://localhost:11434"
ADK_PROJECT_PATH="/content/central_black_extracted/4_agents_source_adk"
COREML_MODEL_PATH="/content/central_black_extracted/3_models_local/llama3.mlpackage"
""")

# Escribir config.py
with open("/content/central_black_extracted/2_backend_orchestrator/app/core/config.py", "w", encoding="utf-8") as f:
    f.write("""import os
from dotenv import load_dotenv
from pathlib import Path

env_path = Path('.') / '.env'
load_dotenv(dotenv_path=env_path)

class Settings:
   GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "")
   OLLAMA_HOST: str = os.getenv("OLLAMA_HOST", "http://localhost:11434")
   ADK_PROJECT_PATH: str = os.getenv("ADK_PROJECT_PATH", "")
   COREML_MODEL_PATH: str = os.getenv("COREML_MODEL_PATH", "")

settings = Settings()
""")

# Escribir orchestrate.py (Schemas)
with open("/content/central_black_extracted/2_backend_orchestrator/app/schemas/orchestrate.py", "w", encoding="utf-8") as f:
    f.write("""from pydantic import BaseModel
from typing import List, Optional

class Message(BaseModel):
   role: str
   content: str
   source: Optional[str] = None

class OrchestrateRequest(BaseModel):
   prompt: str
   history: List[Message]

class OrchestrateResponse(BaseModel):
   reply: str
   source: str
""")

# Escribir gemini_service.py
with open("/content/central_black_extracted/2_backend_orchestrator/app/services/gemini_service.py", "w", encoding="utf-8") as f:
    f.write("""import google.generativeai as genai
from app.core.config import settings

genai.configure(api_key=settings.GEMINI_API_KEY)


model = genai.GenerativeModel('gemini-1.5-flash')

async def generate_gemini_response(prompt: str, history: list) -> str:
   try:
       chat_session = model.start_chat(history=history)
       response = await chat_session.send_message_async(prompt)
       return response.text
   except Exception as e:
       return f"Error al interactuar con Gemini: {e}"
""")

# Escribir ollama_service.py
with open("/content/central_black_extracted/2_backend_orchestrator/app/services/ollama_service.py", "w", encoding="utf-8") as f:
    f.write("""import requests
import json
from app.core.config import settings

async def query_ollama(model: str, prompt: str) -> str:
   url = f"{settings.OLLAMA_HOST}/api/generate"
   payload = {
       "model": model,
       "prompt": prompt,
       "stream": False
   }
   try:
       response = requests.post(url, json=payload)
       response.raise_for_status()
       full_response = ""
       if isinstance(response.json(), dict) and "response" in response.json():
           full_response = response.json()["response"]
       else:
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    full_response += data.get("response", "")
       return full_response
   except requests.exceptions.RequestException as e:
       return f"Error al conectar con Ollama en {url}: {e}"
   except json.JSONDecodeError:
        return f"Error: La respuesta de Ollama no es un JSON válido. Respuesta: {response.text}"
""")

# Escribir coreml_service.py
with open("/content/central_black_extracted/2_backend_orchestrator/app/services/coreml_service.py", "w", encoding="utf-8") as f:
    f.write("""async def query_coreml_llama3(prompt: str) -> str:
   return f"Respuesta simulada de Llama3-CoreML para el prompt: '{prompt}'"
""")

# Escribir huggingface_service.py
with open("/content/central_black_extracted/2_backend_orchestrator/app/services/huggingface_service.py", "w", encoding="utf-8") as f:
    f.write("""from transformers import pipeline

classifier = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

async def query_huggingface(text: str, model_name: str = None) -> str:
    if model_name:
        try:
            new_pipeline = pipeline("text-generation", model=model_name)
            result = new_pipeline(text)
            return result[0]['generated_text']
        except Exception as e:
            return f"Error al cargar/ejecutar modelo Hugging Face '{model_name}': {e}"
    else:
        result = classifier(text)
        return f"Sentimiento: {result[0]['label']}, Score: {result[0]['score']:.2f}"

""")

# Escribir app_agent_orchestrator.py
with open("/content/central_black_extracted/2_backend_orchestrator/agents/app_agent_orchestrator.py", "w", encoding="utf-8") as f:
    f.write("""from app.services import gemini_service, ollama_service, coreml_service
from app.services import huggingface_service
from app.schemas.orchestrate import Message
from typing import List

async def route_prompt(prompt: str, history: List[Message]) -> (str, str):
   prompt_lower = prompt.lower()

   if "revisa" in prompt_lower or "audita" in prompt_lower:
       return f"Respuesta de LLM Auditor (simulado) para: '{prompt}'", "LLM Auditor"

   elif "busca en google" in prompt_lower or "qué es" in prompt_lower:
       return f"Respuesta de Google Search (simulado) para: '{prompt}'", "Google Search Agent"

   elif "código" in prompt_lower or "programa" in prompt_lower or "script" in prompt_lower:
       response = await ollama_service.query_ollama("codellama", prompt)
       return response, "CodeLlama (Ollama)"

   elif "llama3" in prompt_lower:
       response = await coreml_service.query_coreml_llama3(prompt)
       return response, "Llama3 (CoreML)"

   elif "qwen" in prompt_lower:
       response = await ollama_service.query_ollama("qwen", prompt)
       return response, "Qwen (Ollama)"

   elif "huggingface" in prompt_lower or "hf" in prompt_lower:
       parts = prompt_lower.split("huggingface:", 1)
       model_and_text = parts[1].strip() if len(parts) > 1 else ""

       model_name = None
       text_for_hf = prompt

       if " " in model_and_text:
           model_name_candidate, text_for_hf = model_and_text.split(" ", 1)
           model_name = model_name_candidate
       elif model_and_text:
           model_name = model_and_text
           text_for_hf = prompt

       response = await huggingface_service.query_huggingface(text_for_hf, model_name)
       return response, f"Hugging Face ({model_name if model_name else 'Default'})"

   else:
       response = await gemini_service.generate_gemini_response(prompt, history)
       return response, "Gemini Flash"
""")

# Escribir orchestrate.py (Endpoint)
with open("/content/central_black_extracted/2_backend_orchestrator/app/api/v1/endpoints/orchestrate.py", "w", encoding="utf-8") as f:
    f.write("""from fastapi import APIRouter, HTTPException
from app.schemas.orchestrate import OrchestrateRequest, OrchestrateResponse
from agents.app_agent_orchestrator import route_prompt

router = APIRouter()

@router.post("", response_model=OrchestrateResponse)
async def handle_orchestration(request: OrchestrateRequest):
   try:
       reply, source = await route_prompt(request.prompt, request.history)
       return OrchestrateResponse(reply=reply, source=source)
   except Exception as e:
       import traceback
       traceback.print_exc()
       raise HTTPException(status_code=500, detail=str(e))
""")

# Escribir main.py
with open("/content/central_black_extracted/2_backend_orchestrator/app/main.py", "w", encoding="utf-8") as f:
    f.write("""from fastapi import FastAPI
from app.api.v1.endpoints import orchestrate
import sys
import os

app = FastAPI(title="Centralblack Orchestrator API")

app.include_router(orchestrate.router, prefix="/api/v1/orchestrate", tags=["Orchestrator"])

@app.get("/")
def read_root():
   return {"status": "Centralblack Orchestrator está activo."}
""")

"""**5. Ejecutar el Backend con Uvicorn y ngrok**

**IMPORTANTE:** Esta celda iniciará el servidor y ngrok. Debe permanecer ejecutándose. Si necesitas detenerlo, interrumpe la ejecución de esta celda (botón cuadrado de "Stop" o "Interrupt execution").
"""

import asyncio
import uvicorn
from pyngrok import ngrok
import os
import threading
import time

backend_dir = "/content/central_black_extracted/2_backend_orchestrator"
if not os.path.exists(backend_dir):
    print(f"Error: Backend directory not found at {backend_dir}. Please run the previous cells.")
else:
    os.chdir(backend_dir)
    print(f"Cambiado al directorio del backend: {os.getcwd()}")

    def run_uvicorn():
        config = uvicorn.Config("app.main:app", host="127.0.0.1", port=8000, log_level="info")
        server = uvicorn.Server(config)
        server.run()

    uvicorn_thread = threading.Thread(target=run_uvicorn)
    uvicorn_thread.daemon = True
    uvicorn_thread.start()

    time.sleep(5)

    if os.getenv("NGROK_AUTH_TOKEN"):
        try:
            ngrok.kill()
            # Se usa el dominio estático proporcionado por el usuario
            public_url = ngrok.connect(8000, domain="sound-gnu-secondly.ngrok-free.app").public_url
            print(f"\nBackend corriendo y expuesto en: {public_url}")
            print("Este es el endpoint que el frontend debe usar.")
            print("\nLa celda está ejecutando el servidor. Para detenerlo, interrumpe la ejecución de esta celda.")

            while uvicorn_thread.is_alive():
                time.sleep(1)

        except Exception as e:
            print(f"\nError al iniciar ngrok: {e}")
            print("Asegúrate de que tu authtoken de ngrok sea válido y esté configurado en Colab Secrets.")
            print("Verifica que el dominio 'sound-gnu-secondly.ngrok-free.app' esté correctamente configurado en tu cuenta de ngrok y asociado al túnel.")
            print("También verifica si hay otros procesos usando el puerto 8000.")
    else:
        print("\nNGROK_AUTH_TOKEN no está configurado. No se puede iniciar ngrok.")

"""**6. Probar el Endpoint del Backend**

Ejecuta esta celda *después* de que la celda anterior muestre la URL pública de ngrok y confirme que el backend está corriendo.

**Importante:** Si la URL de ngrok cambia (por ejemplo, al reiniciar la celda anterior), deberás actualizar la variable `NGROK_PUBLIC_URL` aquí con la nueva URL.
"""

import requests
import json
import os
import time

time.sleep(5)

# Usar el dominio estático proporcionado por el usuario
NGROK_PUBLIC_URL = "https://sound-gnu-secondly.ngrok-free.app"

if NGROK_PUBLIC_URL:
    ORCHESTRATE_ENDPOINT = f"{NGROK_PUBLIC_URL}/api/v1/orchestrate"

    test_prompt = "¿Cuál es la capital de Francia?"
    request_data = {
        "prompt": test_prompt,
        "history": []
    }

    print(f"Enviando solicitud a: {ORCHESTRATE_ENDPOINT}")
    print(f"Prompt de prueba: {test_prompt}")

    try:
        response = requests.post(ORCHESTRATE_ENDPOINT, json=request_data, timeout=30)

        response.raise_for_status()

        response_json = response.json()
        print("\nRespuesta recibida:")
        print(json.dumps(response_json, indent=2))

        if "reply" in response_json and "source" in response_json:
            print("\nPrueba de backend EXITOSA:")
            print(f"Reply: {response_json['reply']}")
            print(f"Source: {response_json['source']}")
        else:
            print("\nPrueba de backend con ADVERTENCIA: La estructura de la respuesta no es la esperada.")
            print("Esperaba 'reply' y 'source' en el JSON de respuesta.")


    except requests.exceptions.RequestException as e:
        print(f"\nError al enviar la solicitud de prueba: {e}")
        print("Asegúrate de que:")
        print("1. La celda que inicia el servidor Uvicorn y ngrok esté *corriendo* y no haya fallado.")
        print("2. La URL de ngrok que se intentó capturar sea correcta y esté activa.")
        print("3. No haya errores en el código del backend que impidan que responda.")
        print("4. El timeout de la solicitud (actualmente 30 segundos) sea suficiente.")

else:
    print("\nNo se pudo realizar la prueba del backend porque la URL de ngrok no es válida.")
    print("Por favor, asegúrate de que la celda anterior se ejecute correctamente y muestre la URL de ngrok.")

"""**7. Configurar el Frontend (Paso Manual para el Usuario)**

Este paso sigue siendo manual.

1.  Toma la URL pública generada por ngrok en la celda anterior (Paso 5).
2.  Abre el archivo `Central_black/1_frontend_codeforge/vite.config.ts` en tu **máquina local**.
3.  Modifica el bloque `server.proxy` para que apunte a la URL de ngrok.

Ejemplo de cómo `vite.config.ts` debería verse (reemplaza la URL de ejemplo con la tuya):

**8. Finish Task**

Hemos limpiado y organizado el cuaderno para ejecutar tu backend API.

**Para ejecutar tu proyecto:**

1.  Ejecuta todas las celdas de este cuaderno en orden, de arriba a abajo.
2.  La celda "5. Ejecutar el Backend con Uvicorn y ngrok" mostrará la URL pública de ngrok y **se quedará ejecutando** el servidor backend. Déjala así.
3.  La celda "6. Probar el Endpoint del Backend" intentará comunicarse con tu backend a través de la URL de ngrok para verificar que funciona.
4.  Sigue las instrucciones en la celda "7. Configurar el Frontend (Paso Manual para el Usuario)" para actualizar tu frontend local con la URL de ngrok.
5.  Inicia tu frontend localmente. Debería poder comunicarse con el backend que se ejecuta en Colab a través de ngrok.

Si encuentras algún error, revisa la salida de cada celda para identificar el problema (por ejemplo, errores de instalación, ngrok no autenticado, errores en el código del backend). Si la celda de prueba del backend falla, asegúrate de que la celda que inicia el servidor (Paso 5) aún esté ejecutándose y que la URL de ngrok sea correcta.
"""

from google.colab import drive
drive.mount('/content/drive')